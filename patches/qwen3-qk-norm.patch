diff --git a/gguf-py/gguf/constants.py b/gguf-py/gguf/constants.py
index e0abfccb..2a92f917 100644
--- a/gguf-py/gguf/constants.py
+++ b/gguf-py/gguf/constants.py
@@ -1156,6 +1156,9 @@ MODEL_TENSORS: dict[MODEL_ARCH, list[MODEL_TENSOR]] = {
         MODEL_TENSOR.TOKEN_EMBD,
         MODEL_TENSOR.OUTPUT_NORM,
         MODEL_TENSOR.ATTN_NORM,
+        # added for qwen3 support
+        MODEL_TENSOR.ATTN_Q_NORM,
+        MODEL_TENSOR.ATTN_K_NORM,
         MODEL_TENSOR.ATTN_OUT,
         MODEL_TENSOR.FFN_NORM,
         MODEL_TENSOR.FFN_GATE,
diff --git a/gguf-py/gguf/tensor_mapping.py b/gguf-py/gguf/tensor_mapping.py
index cfa6e534..1179550e 100644
--- a/gguf-py/gguf/tensor_mapping.py
+++ b/gguf-py/gguf/tensor_mapping.py
@@ -207,6 +207,15 @@ class TensorNameMap:
             "transformer.h.{bid}.attn.attention.out_proj",                  # exaone
         ),
 
+        # qk norm for qwen3 support
+        MODEL_TENSOR.ATTN_Q_NORM: (
+            "model.layers.{bid}.self_attn.q_norm",                            # qwen3
+        ),
+
+        MODEL_TENSOR.ATTN_K_NORM: (
+            "model.layers.{bid}.self_attn.k_norm",                            # qwen3
+        ),
+
         # Attention output norm
         MODEL_TENSOR.ATTN_OUT_NORM: (
             "encoder.layer.{bid}.attention.output.LayerNorm",  # bert
diff --git a/src/llama.cpp b/src/llama.cpp
index 666fcc4d..0fc974ba 100644
--- a/src/llama.cpp
+++ b/src/llama.cpp
@@ -1337,11 +1337,13 @@ static const std::map<llm_arch, std::map<llm_tensor, const char *>> LLM_TENSOR_N
         {
             { LLM_TENSOR_TOKEN_EMBD,         "token_embd" },
             { LLM_TENSOR_OUTPUT_NORM,        "output_norm" },
+            { LLM_TENSOR_ATTN_NORM,          "blk.%d.attn_norm" },
+            { LLM_TENSOR_ATTN_Q_NORM,        "blk.%d.attn_q_norm" },
+            { LLM_TENSOR_ATTN_K_NORM,        "blk.%d.attn_k_norm" },
             { LLM_TENSOR_ATTN_Q,             "blk.%d.attn_q" },
             { LLM_TENSOR_ATTN_K,             "blk.%d.attn_k" },
             { LLM_TENSOR_ATTN_V,             "blk.%d.attn_v" },
             { LLM_TENSOR_ATTN_OUT,           "blk.%d.attn_output" },
-            { LLM_TENSOR_ATTN_NORM,          "blk.%d.attn_norm" },
             { LLM_TENSOR_ATTN_SUB_NORM,      "blk.%d.attn_sub_norm" },
             { LLM_TENSOR_FFN_GATE,           "blk.%d.ffn_gate" },
             { LLM_TENSOR_FFN_DOWN,           "blk.%d.ffn_down" },
@@ -8690,6 +8692,9 @@ static bool llm_load_tensors(
                         auto & layer = model.layers[i];
 
                         layer.attn_norm     = ml.create_tensor(ctx_layer, tn(LLM_TENSOR_ATTN_NORM,     "weight", i), {n_embd});
+                        // adding qk norm here for qwen3 support, optional for backwards compat
+                        layer.attn_q_norm   = ml.create_tensor(ctx_layer, tn(LLM_TENSOR_ATTN_Q_NORM,  "weight", i), {n_embd_head_k, n_head});
+                        layer.attn_k_norm   = ml.create_tensor(ctx_layer, tn(LLM_TENSOR_ATTN_K_NORM,  "weight", i), {n_embd_head_k, n_head_kv});
                         layer.attn_sub_norm = ml.create_tensor(ctx_layer, tn(LLM_TENSOR_ATTN_SUB_NORM, "weight", i), {n_embd});
 
                         layer.wq       = ml.create_tensor(ctx_split, tn(LLM_TENSOR_ATTN_Q,   "weight", i), {n_embd, n_embd});
@@ -15274,6 +15279,22 @@ struct llm_build_context {
                     cb(Qcur, "Qcur", il);
                 }
 
+                // Apply q_norm if present (Qwen3 style per-head normalization)
+                if (model.layers[il].attn_q_norm) {
+                    Qcur = ggml_view_3d(ctx0, Qcur, n_embd_head, n_head, n_tokens,
+                                ggml_element_size(Qcur) * n_embd_head,
+                                ggml_element_size(Qcur) * n_embd_head * n_head,
+                                0);
+                    cb(Qcur, "Qcur", il);
+                    Qcur = llm_build_norm(ctx0, Qcur, hparams,
+                                model.layers[il].attn_q_norm,
+                                model.layers[il].attn_q_norm_b,
+                                LLM_NORM, cb, il);
+                    cb(Qcur, "Qcur", il);
+                    Qcur = ggml_reshape_2d(ctx0, Qcur, n_embd, n_tokens);
+                    cb(Qcur, "Qcur", il);
+                }
+
                 // B1.K
                 struct ggml_tensor * Kcur = llm_build_lora_mm(lctx, ctx0, model.layers[il].wk, cur);
                 if (model.layers[il].wk_scale) {
@@ -15285,6 +15306,22 @@ struct llm_build_context {
                     cb(Kcur, "Kcur", il);
                 }
 
+                // Apply k_norm if present (Qwen3 style per-head normalization)
+                if (model.layers[il].attn_k_norm) {
+                    Kcur = ggml_view_3d(ctx0, Kcur, n_embd_head, n_head_kv, n_tokens,
+                                ggml_element_size(Kcur) * n_embd_head,
+                                ggml_element_size(Kcur) * n_embd_head * n_head_kv,
+                                0);
+                    cb(Kcur, "Kcur", il);
+                    Kcur = llm_build_norm(ctx0, Kcur, hparams,
+                                model.layers[il].attn_k_norm,
+                                model.layers[il].attn_k_norm_b,
+                                LLM_NORM, cb, il);
+                    cb(Kcur, "Kcur", il);
+                    Kcur = ggml_reshape_2d(ctx0, Kcur, hparams.n_embd_k_gqa(il), n_tokens);
+                    cb(Kcur, "Kcur", il);
+                }
+
                 // B1.V
                 struct ggml_tensor * Vcur = llm_build_lora_mm(lctx, ctx0, model.layers[il].wv, cur);
                 if (model.layers[il].wv_scale) {
