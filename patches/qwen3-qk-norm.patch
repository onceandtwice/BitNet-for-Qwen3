diff --git a/src/llama.cpp b/src/llama.cpp
index 666fcc4d..9859cf48 100644
--- a/src/llama.cpp
+++ b/src/llama.cpp
@@ -1337,11 +1337,13 @@ static const std::map<llm_arch, std::map<llm_tensor, const char *>> LLM_TENSOR_N
         {
             { LLM_TENSOR_TOKEN_EMBD,         "token_embd" },
             { LLM_TENSOR_OUTPUT_NORM,        "output_norm" },
+            { LLM_TENSOR_ATTN_NORM,          "blk.%d.attn_norm" },
+            { LLM_TENSOR_ATTN_Q_NORM,        "blk.%d.attn_q_norm" },
+            { LLM_TENSOR_ATTN_K_NORM,        "blk.%d.attn_k_norm" },
             { LLM_TENSOR_ATTN_Q,             "blk.%d.attn_q" },
             { LLM_TENSOR_ATTN_K,             "blk.%d.attn_k" },
             { LLM_TENSOR_ATTN_V,             "blk.%d.attn_v" },
             { LLM_TENSOR_ATTN_OUT,           "blk.%d.attn_output" },
-            { LLM_TENSOR_ATTN_NORM,          "blk.%d.attn_norm" },
             { LLM_TENSOR_ATTN_SUB_NORM,      "blk.%d.attn_sub_norm" },
             { LLM_TENSOR_FFN_GATE,           "blk.%d.ffn_gate" },
             { LLM_TENSOR_FFN_DOWN,           "blk.%d.ffn_down" },
@@ -8690,6 +8692,9 @@ static bool llm_load_tensors(
                         auto & layer = model.layers[i];
 
                         layer.attn_norm     = ml.create_tensor(ctx_layer, tn(LLM_TENSOR_ATTN_NORM,     "weight", i), {n_embd});
+                        # adding qk norm here for qwen3 support, optional for backwards compat
+                        layer.attn_q_norm   = ml.create_tensor(ctx_layer, tn(LLM_TENSOR_ATTN_Q_NORM,  "weight", i), {n_embd_head_k, n_head}, llama_model_loader::TENSOR_NOT_REQUIRED);
+                        layer.attn_k_norm   = ml.create_tensor(ctx_layer, tn(LLM_TENSOR_ATTN_K_NORM,  "weight", i), {n_embd_head_k, n_head_kv}, llama_model_loader::TENSOR_NOT_REQUIRED);
                         layer.attn_sub_norm = ml.create_tensor(ctx_layer, tn(LLM_TENSOR_ATTN_SUB_NORM, "weight", i), {n_embd});
 
                         layer.wq       = ml.create_tensor(ctx_split, tn(LLM_TENSOR_ATTN_Q,   "weight", i), {n_embd, n_embd});
@@ -15274,6 +15279,22 @@ struct llm_build_context {
                     cb(Qcur, "Qcur", il);
                 }
 
+                // Apply q_norm if present (Qwen3 style per-head normalization)
+                if (model.layers[il].attn_q_norm) {
+                    Qcur = ggml_view_3d(ctx0, Qcur, n_embd_head, n_head, n_tokens,
+                                ggml_element_size(Qcur) * n_embd_head,
+                                ggml_element_size(Qcur) * n_embd_head * n_head,
+                                0);
+                    cb(Qcur, "Qcur", il);
+                    Qcur = llm_build_norm(ctx0, Qcur, hparams,
+                                model.layers[il].attn_q_norm,
+                                model.layers[il].attn_q_norm_b,
+                                LLM_NORM, cb, il);
+                    cb(Qcur, "Qcur", il);
+                    Qcur = ggml_reshape_2d(ctx0, Qcur, n_embd, n_tokens);
+                    cb(Qcur, "Qcur", il);
+                }
+
                 // B1.K
                 struct ggml_tensor * Kcur = llm_build_lora_mm(lctx, ctx0, model.layers[il].wk, cur);
                 if (model.layers[il].wk_scale) {
@@ -15285,6 +15306,22 @@ struct llm_build_context {
                     cb(Kcur, "Kcur", il);
                 }
 
+                // Apply k_norm if present (Qwen3 style per-head normalization)
+                if (model.layers[il].attn_k_norm) {
+                    Kcur = ggml_view_3d(ctx0, Kcur, n_embd_head, n_head_kv, n_tokens,
+                                ggml_element_size(Kcur) * n_embd_head,
+                                ggml_element_size(Kcur) * n_embd_head * n_head_kv,
+                                0);
+                    cb(Kcur, "Kcur", il);
+                    Kcur = llm_build_norm(ctx0, Kcur, hparams,
+                                model.layers[il].attn_k_norm,
+                                model.layers[il].attn_k_norm_b,
+                                LLM_NORM, cb, il);
+                    cb(Kcur, "Kcur", il);
+                    Kcur = ggml_reshape_2d(ctx0, Kcur, n_embd_gqa, n_tokens);
+                    cb(Kcur, "Kcur", il);
+                }
+
                 // B1.V
                 struct ggml_tensor * Vcur = llm_build_lora_mm(lctx, ctx0, model.layers[il].wv, cur);
                 if (model.layers[il].wv_scale) {
